{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importando as bibliotecas:"
      ],
      "metadata": {
        "id": "Vj2NR-V6X38d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThtuhsxEXxqx"
      },
      "outputs": [],
      "source": [
        "# Importando libs para processamento de texto:\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "# Importando utilitários\n",
        "import re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixndo os recursos necessários da nlkt:\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KgY8vGBYp0j",
        "outputId": "72f48b07-f562-4f2c-8dda-3a0d7c8b7e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplos de usos da lib nlkt:"
      ],
      "metadata": {
        "id": "v1DIOG7Un8kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divisão do texto em tokens:\n",
        "\n",
        "texto = \"O processamento de linguagem natural é uma área fascinante da ciência da computação.\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PKGbsR5X-Tw",
        "outputId": "abdd5f75-0ae8-4c6a-f6f0-ce06fe8bd6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'processamento', 'de', 'linguagem', 'natural', 'é', 'uma', 'área', 'fascinante', 'da', 'ciência', 'da', 'computação', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remoção das stop_words:\n",
        "\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "filtered_sentence = [w for w in word_tokenize(texto) if not w.lower() in stop_words]\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGtcdVARYziP",
        "outputId": "b573120e-4953-4635-850e-7b791189a353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['processamento', 'linguagem', 'natural', 'área', 'fascinante', 'ciência', 'computação', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming para reduzir as palavras a sua forma base/raiz: (facilita o processamento dos dados)\n",
        "\n",
        "words = [\"Processing\", \"Processes\", \"Processed\", \"Processor\", \"Procession\", \"Processional\", \"Processable\", \"Processedness\", \"Processibility\", \"Processionally\"]\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAm_4uyd0qHR",
        "outputId": "6f1df448-917b-4d67-8f56-55b34cf0a315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['process', 'process', 'process', 'processor', 'process', 'procession', 'process', 'processed', 'process', 'procession']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análise básica de sentimentos:\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "text = \"I love programming in Python!\"\n",
        "sentiment_score = sia.polarity_scores(text)\n",
        "print(sentiment_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQb4zCuvZ673",
        "outputId": "55f91031-476d-45f7-d43a-c16614d9ab8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.4, 'pos': 0.6, 'compound': 0.6696}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Atribui a cada token uma tag de POS (Part-of-Speech), ou seja sua classe gramatical. sempre vai retornar essa tag em inglês.\n",
        "\n",
        "text = \"O processamento de linguagem natural é uma área fascinante da ciência da computação.\"\n",
        "tagged_words = pos_tag(tokens)\n",
        "print(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1d7PieyaBEE",
        "outputId": "fd701a58-760a-48bc-9e7c-614c94f18877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('O', 'NNP'), ('processamento', 'NN'), ('de', 'FW'), ('linguagem', 'FW'), ('natural', 'JJ'), ('é', 'NNP'), ('uma', 'JJ'), ('área', 'NNP'), ('fascinante', 'NN'), ('da', 'NN'), ('ciência', 'NN'), ('da', 'NN'), ('computação', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processando um texto completo:"
      ],
      "metadata": {
        "id": "SvBIR-4Vn12a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"\"\"No meio do caminho tinha uma pedra\n",
        "tinha uma pedra no meio do caminho\n",
        "tinha uma pedra\n",
        "no meio do caminho tinha uma pedra.\n",
        "Nunca me esquecerei desse acontecimento\n",
        "na vida de minhas retinas tão fatigadas.\n",
        "Nunca me esquecerei que no meio do caminho\n",
        "tinha uma pedra\n",
        "tinha uma pedra no meio do caminho\n",
        "no meio do caminho tinha uma pedra.\"\"\"\n",
        "\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw2D7_JdlFiw",
        "outputId": "6c5cc8b0-16d5-4029-d9e0-7acf64898db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No meio do caminho tinha uma pedra\n",
            "tinha uma pedra no meio do caminho\n",
            "tinha uma pedra\n",
            "no meio do caminho tinha uma pedra.\n",
            "Nunca me esquecerei desse acontecimento\n",
            "na vida de minhas retinas tão fatigadas.\n",
            "Nunca me esquecerei que no meio do caminho\n",
            "tinha uma pedra\n",
            "tinha uma pedra no meio do caminho\n",
            "no meio do caminho tinha uma pedra.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = texto.lower()\n",
        "print(texto)\n",
        "\n",
        "\"\"\"\n",
        "Colocar todas as letras em minúsculo é importante\n",
        "porque nossa máquina tende a interpretar palavras iguais\n",
        "mas com letras minúsculas e maiúsculas como sendo diferentes.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "toTS30culojo",
        "outputId": "c2fdc51a-d4fa-4f53-8c49-4bb330549084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no meio do caminho tinha uma pedra\n",
            "tinha uma pedra no meio do caminho\n",
            "tinha uma pedra\n",
            "no meio do caminho tinha uma pedra.\n",
            "nunca me esquecerei desse acontecimento\n",
            "na vida de minhas retinas tão fatigadas.\n",
            "nunca me esquecerei que no meio do caminho\n",
            "tinha uma pedra\n",
            "tinha uma pedra no meio do caminho\n",
            "no meio do caminho tinha uma pedra.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nColocar todas as letras em minúsculo é importante\\nporque nossa máquina tende a interpretar palavras iguais\\nmas com letras minúsculas e maiúsculas como sendo diferentes.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleciona apenas letras (lembrando que o texto está em português e as letras podem possuir acento)\n",
        "apenas_letras = re.findall(r'[a-zéóáêâãõç]+', texto)\n",
        "\n",
        "print(apenas_letras)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i39UCpXrmVGU",
        "outputId": "089dc730-e15c-4c27-d5a7-ec4417d01075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'nunca', 'me', 'esquecerei', 'desse', 'acontecimento', 'na', 'vida', 'de', 'minhas', 'retinas', 'tão', 'fatigadas', 'nunca', 'me', 'esquecerei', 'que', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Junta o texto, já que o (.findall) separa em tokens\n",
        "novo_texto = \" \".join(apenas_letras)\n",
        "\n",
        "print(novo_texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Euy1s72SnyPD",
        "outputId": "9bc8fd2c-d955-446b-977b-710e86452287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no meio do caminho tinha uma pedra tinha uma pedra no meio do caminho tinha uma pedra no meio do caminho tinha uma pedra nunca me esquecerei desse acontecimento na vida de minhas retinas tão fatigadas nunca me esquecerei que no meio do caminho tinha uma pedra tinha uma pedra no meio do caminho no meio do caminho tinha uma pedra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo BoW (Bag of Words):"
      ],
      "metadata": {
        "id": "P4R_kAwcyv8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gerando o vocabulário:\n",
        "\n",
        "\n",
        "\n",
        "1.   Separar nosso texto em tokens;\n",
        "\n",
        "2.   Criar uma lista para guardarmos o vocabulário;\n",
        "\n",
        "3.   Fazer um loop para percorrer o texto inteiro;\n",
        "\n",
        "4.   Criar uma condicional para verificar se a palavra está na lista —fazemos isso porque nosso vocabulário conta apenas as ocorrências únicas, sem repetições de palavras;\n",
        "\n",
        "5.   Caso não esteja, ela é adicionada."
      ],
      "metadata": {
        "id": "jo3aGCiqpgJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_texto = word_tokenize(novo_texto)"
      ],
      "metadata": {
        "id": "u1KJKm2BpjWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_maker(tokens):\n",
        "  Vocab = []\n",
        "  for token in tokens:\n",
        "      if token not in Vocab:\n",
        "          Vocab.append(token)\n",
        "  return Vocab"
      ],
      "metadata": {
        "id": "PQIQbkVp8W-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vocab_maker(tokens_texto)\n",
        "\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4j4w41o8lCW",
        "outputId": "d6b08bc2-6f26-42d3-d645-cbf18622d9c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'nunca', 'me', 'esquecerei', 'desse', 'acontecimento', 'na', 'vida', 'de', 'minhas', 'retinas', 'tão', 'fatigadas', 'que']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Codificação binária:\n",
        "\n",
        "1. Criar uma lista que representa o vetor;\n",
        "2. Fazer um loop para percorrer todas as palavras do vocabulário;\n",
        "3. Se a palavra estiver no documento, adicionar 1 à lista; caso contrário,\n",
        "   adicionar 0;\n",
        "4. Transformar a lista final em um array do numpy e retornar."
      ],
      "metadata": {
        "id": "iFpddS-pyMUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cria_vetor_documento(documento, vocab):\n",
        "  vetor = []\n",
        "\n",
        "  for palavra in vocab:\n",
        "    if palavra in documento:\n",
        "        vetor.append(1)\n",
        "    else:\n",
        "        vetor.append(0)\n",
        "\n",
        "  return np.array(vetor)"
      ],
      "metadata": {
        "id": "xBPLlZfyuupj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de uso em uma amostra:"
      ],
      "metadata": {
        "id": "99zhRtlWzCxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_amostra = \"no meio do caminho tinha uma pedra\"\n",
        "vetor_doc = cria_vetor_documento(texto_amostra, vocab)\n",
        "\n",
        "print(vetor_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH-NzqxGuyXm",
        "outputId": "cf701c3f-4f28-4ee8-b97b-ec254574b468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency Inverse Document Frequency (TF-IDF):\n",
        "\n",
        "Term Frequency mede a frequência com que um termo ocorre num documento;\n",
        "\n",
        "Inverse Document Frequency mede o quão importante um termo é no contexto de todos os documentos."
      ],
      "metadata": {
        "id": "duf9J3VG2WGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estrofe1= \"\"\"no meio do caminho tinha uma pedra tinha uma pedra no meio do caminho tinha uma pedra no meio do caminho tinha uma pedra\"\"\"\n",
        "\n",
        "estrofe2 = \"\"\"nunca me esquecerei desse acontecimento na vida de minhas retinas tão fatigadas nunca me esquecerei que no meio do caminho tinha uma pedra tinha uma pedra no meio do caminho no meio do caminho tinha uma pedra\"\"\""
      ],
      "metadata": {
        "id": "b5eEAemg2bnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1_tokens = word_tokenize(estrofe1)\n",
        "print(e1_tokens)\n",
        "\n",
        "e2_tokens = word_tokenize(estrofe2)\n",
        "print(e2_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk9C-PCo8Db6",
        "outputId": "fa29846d-16ad-4bdf-9556-98846afb4fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']\n",
            "['nunca', 'me', 'esquecerei', 'desse', 'acontecimento', 'na', 'vida', 'de', 'minhas', 'retinas', 'tão', 'fatigadas', 'nunca', 'me', 'esquecerei', 'que', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_uniao = e1_tokens + e2_tokens\n",
        "print(tokens_uniao)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDzGC4hZaHGg",
        "outputId": "7657b78e-0859-482b-cdff-dee1701bdb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'nunca', 'me', 'esquecerei', 'desse', 'acontecimento', 'na', 'vida', 'de', 'minhas', 'retinas', 'tão', 'fatigadas', 'nunca', 'me', 'esquecerei', 'que', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho', 'no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "novo_vocab = vocab_maker(tokens_uniao)\n",
        "print(novo_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEHeUkNRbfME",
        "outputId": "bfec49c1-5b86-490c-d53d-fb2683f8db2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'nunca', 'me', 'esquecerei', 'desse', 'acontecimento', 'na', 'vida', 'de', 'minhas', 'retinas', 'tão', 'fatigadas', 'que']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Recebe uma lista com o vocabulario e uma lista de tokens de um documento.\n",
        "Retorna um dicionario com o numero de vezes que cada palavra do vocabulario\n",
        "ocorre no documento.'''\n",
        "\n",
        "def dicionario_de_contagem(vocabulario, documento):\n",
        "  dic = dict.fromkeys(vocabulario, 0)\n",
        "  for palavra in documento:\n",
        "    dic[palavra] += 1\n",
        "\n",
        "  return dic"
      ],
      "metadata": {
        "id": "45RTxPm9bvzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1_dic_cont = dicionario_de_contagem(novo_vocab, e1_tokens)\n",
        "e2_dic_cont = dicionario_de_contagem(novo_vocab, e2_tokens)\n",
        "\n",
        "print(e1_dic_cont,'\\n')\n",
        "print(e2_dic_cont)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCwykQVpb3Vf",
        "outputId": "83899fbf-86e7-4cfa-b1bd-a0d47b09fa8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'no': 3, 'meio': 3, 'do': 3, 'caminho': 3, 'tinha': 4, 'uma': 4, 'pedra': 4, 'nunca': 0, 'me': 0, 'esquecerei': 0, 'desse': 0, 'acontecimento': 0, 'na': 0, 'vida': 0, 'de': 0, 'minhas': 0, 'retinas': 0, 'tão': 0, 'fatigadas': 0, 'que': 0} \n",
            "\n",
            "{'no': 3, 'meio': 3, 'do': 3, 'caminho': 3, 'tinha': 3, 'uma': 3, 'pedra': 3, 'nunca': 2, 'me': 2, 'esquecerei': 2, 'desse': 1, 'acontecimento': 1, 'na': 1, 'vida': 1, 'de': 1, 'minhas': 1, 'retinas': 1, 'tão': 1, 'fatigadas': 1, 'que': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculando o Term Frequency:"
      ],
      "metadata": {
        "id": "uKpE8GL7cRWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculaTF(dic_de_cont, doc):\n",
        "  tf_dic = {}\n",
        "\n",
        "  num_palavras_doc = len(doc)\n",
        "  for palavra, contagem in dic_de_cont.items():\n",
        "      tf_dic[palavra] = contagem/float(num_palavras_doc)\n",
        "\n",
        "  return(tf_dic)"
      ],
      "metadata": {
        "id": "F59LpETCcatQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1_tf_bow = calculaTF(e1_dic_cont, e1_tokens)\n",
        "e2_tf_bow = calculaTF(e2_dic_cont, e2_tokens)"
      ],
      "metadata": {
        "id": "AZGTZMFkcj1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e1_tf_bow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6WAiQ_0clrw",
        "outputId": "697dfd11-d082-448e-957f-9f25d10f1569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'no': 0.125,\n",
              " 'meio': 0.125,\n",
              " 'do': 0.125,\n",
              " 'caminho': 0.125,\n",
              " 'tinha': 0.16666666666666666,\n",
              " 'uma': 0.16666666666666666,\n",
              " 'pedra': 0.16666666666666666,\n",
              " 'nunca': 0.0,\n",
              " 'me': 0.0,\n",
              " 'esquecerei': 0.0,\n",
              " 'desse': 0.0,\n",
              " 'acontecimento': 0.0,\n",
              " 'na': 0.0,\n",
              " 'vida': 0.0,\n",
              " 'de': 0.0,\n",
              " 'minhas': 0.0,\n",
              " 'retinas': 0.0,\n",
              " 'tão': 0.0,\n",
              " 'fatigadas': 0.0,\n",
              " 'que': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e2_tf_bow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpP2oks-cuLb",
        "outputId": "8645d132-b998-416e-cf2b-60a5b798a3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'no': 0.08108108108108109,\n",
              " 'meio': 0.08108108108108109,\n",
              " 'do': 0.08108108108108109,\n",
              " 'caminho': 0.08108108108108109,\n",
              " 'tinha': 0.08108108108108109,\n",
              " 'uma': 0.08108108108108109,\n",
              " 'pedra': 0.08108108108108109,\n",
              " 'nunca': 0.05405405405405406,\n",
              " 'me': 0.05405405405405406,\n",
              " 'esquecerei': 0.05405405405405406,\n",
              " 'desse': 0.02702702702702703,\n",
              " 'acontecimento': 0.02702702702702703,\n",
              " 'na': 0.02702702702702703,\n",
              " 'vida': 0.02702702702702703,\n",
              " 'de': 0.02702702702702703,\n",
              " 'minhas': 0.02702702702702703,\n",
              " 'retinas': 0.02702702702702703,\n",
              " 'tão': 0.02702702702702703,\n",
              " 'fatigadas': 0.02702702702702703,\n",
              " 'que': 0.02702702702702703}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculando o Inverse Document Frequency:"
      ],
      "metadata": {
        "id": "Oovs0o9sdlKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computaIDF(lista_de_docs):\n",
        "    idf_dic = {}\n",
        "    N = len(lista_de_docs)\n",
        "\n",
        "    for palavra in lista_de_docs[0]:\n",
        "        num_docs_aparece = 0\n",
        "        for doc in lista_de_docs:\n",
        "            if doc[palavra]>0:\n",
        "                num_docs_aparece += 1\n",
        "\n",
        "        idf_dic[palavra] = math.log10(N / (num_docs_aparece))\n",
        "\n",
        "    return (idf_dic)"
      ],
      "metadata": {
        "id": "JzY_83j2dql_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estrofes_idf = computaIDF([e1_dic_cont, e2_dic_cont])\n",
        "print(estrofes_idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju-KjCp6oXKF",
        "outputId": "922f13e1-b376-4571-f6f4-92a93cd6c720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'no': 0.0, 'meio': 0.0, 'do': 0.0, 'caminho': 0.0, 'tinha': 0.0, 'uma': 0.0, 'pedra': 0.0, 'nunca': 0.3010299956639812, 'me': 0.3010299956639812, 'esquecerei': 0.3010299956639812, 'desse': 0.3010299956639812, 'acontecimento': 0.3010299956639812, 'na': 0.3010299956639812, 'vida': 0.3010299956639812, 'de': 0.3010299956639812, 'minhas': 0.3010299956639812, 'retinas': 0.3010299956639812, 'tão': 0.3010299956639812, 'fatigadas': 0.3010299956639812, 'que': 0.3010299956639812}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Sequence:"
      ],
      "metadata": {
        "id": "FRZxQXeHl-5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulário para nossa tokenização:\n",
        "palavras = ['gato', 'comeu', 'peixe', 'mesa', 'cachorro', 'da', 'mesa', 'osso', 'mordeu', 'o', 'um', 'na', 'enquanto']\n",
        "\n",
        "# Tamanho do vocabulário (pode definir a dimensão de imput do embeddings dentro de um modelo)\n",
        "tam_vocab = len(palavras)\n",
        "\n",
        "# Frases para testar nossa tokenização\n",
        "frases = []\n",
        "frases.append('o gato comeu um peixe na mesa')\n",
        "frases.append('o cachorro mordeu o osso')\n",
        "\n",
        "# Instânciando o tokenizador e dando o vocabulário de entrada\n",
        "tokenizer = Tokenizer(num_words=tam_vocab)\n",
        "tokenizer.fit_on_texts(palavras)\n",
        "\n",
        "# Tokenizando os textos:\n",
        "frases_tokenizadas = tokenizer.texts_to_sequences(frases)"
      ],
      "metadata": {
        "id": "PHHzF2AJmCiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando a estrutura do tokenizer\n",
        "print(\"Índices das palavras:\")\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5V51QIfOtd-",
        "outputId": "f00efec4-c7ac-45ff-f3b1-aa396a182922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índices das palavras:\n",
            "{'mesa': 1, 'gato': 2, 'comeu': 3, 'peixe': 4, 'cachorro': 5, 'da': 6, 'osso': 7, 'mordeu': 8, 'o': 9, 'um': 10, 'na': 11, 'enquanto': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verificando e corrindo a dimensionalidade dos vetores(frases):"
      ],
      "metadata": {
        "id": "b3oX5b-xqftD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases_tokenizadas[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqSPUyE-ojpl",
        "outputId": "c98c5cc9-a0fc-4026-cbfc-48be58f3fb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9, 2, 3, 10, 4, 11, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando os vetores das frases:\n",
        "frases_tokenizadas[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DkvVfEPoXD2",
        "outputId": "357855a6-488d-4c88-c117-f42ab19d38e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9, 5, 8, 9, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando a frase com maior número de tokens:\n",
        "maior = max(len(tokens) for tokens in frases_tokenizadas)\n",
        "print(\"Maior frase:\", maior)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Uw5FLRao9iB",
        "outputId": "efc3fbcd-cb57-42d0-a3b7-e21c4d12f763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maior frase: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performando padding para deixar todas as \"frases\" com o mesmo tamanho: (adiciona 0 para completar o vetor)\n",
        "frases_tokenizadas_padding = pad_sequences(frases_tokenizadas, maxlen=maior, padding='post',)"
      ],
      "metadata": {
        "id": "KsyM_T5DpMMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frases_tokenizadas_padding[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wet3b7Scpn36",
        "outputId": "09aa5670-07cf-4a36-b292-048eea5adb97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9,  2,  3, 10,  4, 11,  1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frases_tokenizadas_padding[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdG4KzwlptdY",
        "outputId": "2be6bf0a-82c2-4e3b-af2c-6256b39acb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 5, 8, 9, 7, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-Embeddings: (word2vec)\n",
        "\n",
        "Word2Vec é um modelo de aprendizado de máquina usado para representar palavras como vetores numéricos densos. Ele captura relações semânticas e sintáticas entre palavras. O modelo utiliza uma rede neural para aprender representações distribuídas de palavras a partir de contextos onde elas aparecem."
      ],
      "metadata": {
        "id": "6BIYCzxprwiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto de exemplo\n",
        "texto_exemplo = \"I love coding. Word embeddings are awesome. Machine learning is fun.\"\n",
        "\n",
        "# Tokenização\n",
        "tokens_exemplo = re.findall(r'[a-zA-Z]+', texto_exemplo)\n",
        "print(\"Tokens:\", tokens_exemplo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7uxKxZdr0Do",
        "outputId": "0be4be06-ff9b-4655-f866-e2d687a48c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['I', 'love', 'coding', 'Word', 'embeddings', 'are', 'awesome', 'Machine', 'learning', 'is', 'fun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tipo do word2vec para se utilizar: (0) cbow - (1) skip-gram\n",
        "tipo = 1\n",
        "\n",
        "# Treinamento do modelo Word2Vec\n",
        "model = Word2Vec([tokens_exemplo], vector_size=100, window=5, min_count=1, workers=4, sg=tipo)"
      ],
      "metadata": {
        "id": "qlIul2GcsIvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter o vetor de uma palavra\n",
        "vector = model.wv['coding']\n",
        "\n",
        "print(\"Vetor de 'coding':\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rMU0aOosv5N",
        "outputId": "674bffb5-f8f3-4cb9-d1bf-25a4a28350fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vetor de 'coding': [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
            "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
            " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
            "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
            " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
            "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
            "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
            "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
            "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
            " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
            " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
            " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
            " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
            " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
            "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
            " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
            " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
            " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
            "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
            " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
            " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
            "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
            " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
            "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
            "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar palavras similares\n",
        "similar_words = model.wv.most_similar('coding')\n",
        "\n",
        "print(\"Palavras similares a 'coding':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqrR5aSHszDe",
        "outputId": "12aaeb0c-cd4b-4110-9bb4-1388db05b732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras similares a 'coding': [('learning', 0.19912061095237732), ('Machine', 0.07497556507587433), ('are', 0.060591842979192734), ('love', 0.04469989985227585), ('is', 0.03364057466387749), ('fun', 0.027057481929659843), ('I', 0.026806799694895744), ('embeddings', 0.008826158009469509), ('awesome', -0.06900332123041153), ('Word', -0.14454564452171326)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Realizando um novo treinamento sob o nosso modelo:"
      ],
      "metadata": {
        "id": "RrwVhV7ayJ6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento com mais dados (opcional)\n",
        "mais_texto = \"Deep learning is a powerful technique. Natural language processing is fascinating.\"\n",
        "mais_tokens = re.findall(r'[a-zA-Z]+', mais_texto)\n",
        "\n",
        "model.build_vocab([mais_tokens], update=True)\n",
        "model.train([mais_tokens], total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIMK0u1yt6mM",
        "outputId": "cad7eecd-eb0d-47af-a5c7-ba060a593629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 55)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar palavras similares\n",
        "similar_words = model.wv.most_similar('coding')\n",
        "print(\"Palavras similares a 'coding':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAc--zPjugkP",
        "outputId": "5bc02a83-2943-4a93-92fb-6007bf708eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras similares a 'coding': [('learning', 0.19912061095237732), ('Machine', 0.07497556507587433), ('are', 0.060591842979192734), ('love', 0.04469989985227585), ('Deep', 0.0377129502594471), ('is', 0.03364057466387749), ('fun', 0.027057481929659843), ('I', 0.026806797832250595), ('fascinating', 0.015797005966305733), ('processing', 0.01243623998016119)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvando nosso modelo:"
      ],
      "metadata": {
        "id": "22J77miNyGEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar modelo treinado\n",
        "model.save(\"word2vec_model.bin\")\n",
        "\n",
        "# Carregar modelo treinado\n",
        "loaded_model = Word2Vec.load(\"word2vec_model.bin\")"
      ],
      "metadata": {
        "id": "TxABJ_uzuPUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculando algumas propriedades:"
      ],
      "metadata": {
        "id": "0vcMcwPByBIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando a similaridade entre duas palavras\n",
        "similarity_score = loaded_model.wv.similarity('Deep', 'learning')\n",
        "\n",
        "print(\"Similaridade entre as palavras escolhidas é: \", similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1Gs7BLlus9m",
        "outputId": "13c24f98-91d5-4d45-85ab-ab14ae9c08a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridade entre as palavras escolhidas é:  0.17272793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculando a distância entre dois vetores de palavra\n",
        "distance = loaded_model.wv.distance('Deep', 'learning')\n",
        "print(\"Distância entre as palavras escolhidas é: \", distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUTDZzzYxXwQ",
        "outputId": "b4709e71-5c36-44ae-bc24-b740a6ae105f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distância entre as palavras escolhidas é:  0.8272720724344254\n"
          ]
        }
      ]
    }
  ]
}